# 1. Introduction

In recent years, the integration of 3D vision and generative models has become a research hotspot in deep learning. Diffusion Models [1], with their powerful generative abilities, have gradually expanded from 2D image tasks to 3D data generation tasks [2]. In terms of 3D representation, voxels, as a regular and structured data form, provide similar processing convenience as 2D grids for diffusion models, showing potential in tasks such as scene reconstruction and object detection [3] [4].

However, current methods still face several challenges. Firstly, most 3D diffusion models focus on single-object generation, making it difficult to effectively construct complex scenes involving multiple interacting objects [5]. Secondly, the choice of 3D data representation directly affects the efficiency and accuracy of models [2]. Lastly, the integration of generative models with downstream tasks (such as object detection and drug design) remains weak, and the mechanism for incorporating label information urgently needs optimization [6].

This project aims to design a simple voxel-based 3D diffusion model and explore the generation of labeled 3D models. By combining the regularity of voxel representation with the generative capabilities of diffusion models and embedding label information, the model's adaptability to semantic constraints is enhanced, providing data support for downstream tasks (such as 3D scene understanding and molecular property prediction). The implementation of this project will push the boundaries of diffusion models in the realm of 3D generation, offer new ideas for multi-object scene modeling and text-prompt based generative modeling, and provide a lightweight, reproducible 3D generation framework for educational practice.