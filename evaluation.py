import torch
import numpy as np
from torch.utils.data import DataLoader
from scipy.ndimage import label as ndlabel

from module.dataset import DummyDataset
from module.diffusion import Diffusion
from module.unet import VoxelUNet


def evaluate_voxel_quality(model, diffusion, dataset, batch_size=16, num_batches=10, only_mask=False, device='cuda'):
    """
    Evaluates the quality of voxels generated by a diffusion model using multiple metrics.

    Returns a dictionary with various quality metrics.
    """
    model.eval()
    metrics = {}

    # 1. Generate samples
    real_samples = []
    real_labels = []
    generated_samples = []
    generated_labels = []

    # Get real samples from dataset
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    for i, (voxel, label) in enumerate(dataloader):
        if i >= num_batches:
            break
        if only_mask:
            voxel = voxel[:, 3:, ...]
        real_samples.append(voxel.cpu())
        real_labels.extend(label.cpu().numpy())

    real_samples = torch.cat(real_samples, dim=0)

    # Generate samples using the model
    for i in range(num_batches):
        # Generate with random labels
        labels = torch.randint(0, dataset.num_classes, (batch_size,), device=device)
        samples = diffusion.ddim_sample(
            model,
            batch_size=batch_size,
            channels=1 if only_mask else 4,
            size=model.model_channels,
            label=labels
        )
        generated_samples.append(samples.cpu())
        generated_labels.extend(labels.cpu().numpy())

    generated_samples = torch.cat(generated_samples, dim=0)

    # 2. Calculate structural metrics
    metrics.update(compute_structural_metrics(real_samples, generated_samples, only_mask))

    # 3. Calculate distribution metrics
    metrics.update(compute_distribution_metrics(real_samples, generated_samples, only_mask))

    # 4. Calculate classification accuracy (if we had a classifier)
    metrics.update(compute_classification_metrics(real_labels, generated_labels))

    return metrics


def compute_structural_metrics(real_samples, generated_samples, only_mask=False):
    """Calculate metrics related to structural properties of the voxels"""
    metrics = {}

    # Permute for easier processing
    real_samples = real_samples.permute(0, 2, 3, 4, 1)
    generated_samples = generated_samples.permute(0, 2, 3, 4, 1)

    # Extract occupancy masks
    if only_mask:
        real_masks = (real_samples[..., 0] > 0).float()
        gen_masks = (generated_samples[..., 0] > 0).float()
    else:
        real_masks = (real_samples[..., 3] > 0).float()
        gen_masks = (generated_samples[..., 3] > 0).float()

    # 1. Occupancy rate
    real_occupancy = real_masks.mean(dim=[1, 2, 3]).numpy()
    gen_occupancy = gen_masks.mean(dim=[1, 2, 3]).numpy()

    metrics['real_occupancy_mean'] = float(np.mean(real_occupancy))
    metrics['gen_occupancy_mean'] = float(np.mean(gen_occupancy))
    metrics['occupancy_diff'] = float(abs(np.mean(real_occupancy) - np.mean(gen_occupancy)))

    # 2. Connected components analysis (on a subset for efficiency)
    real_components = []
    gen_components = []

    for i in range(min(20, len(real_masks))):
        real_voxel = real_masks[i].numpy()
        labeled, num_components = ndlabel(real_voxel)
        real_components.append(num_components)

        gen_voxel = gen_masks[i].numpy()
        labeled, num_components = ndlabel(gen_voxel)
        gen_components.append(num_components)

    metrics['real_components_mean'] = float(np.mean(real_components))
    metrics['gen_components_mean'] = float(np.mean(gen_components))
    metrics['components_diff'] = float(abs(np.mean(real_components) - np.mean(gen_components)))

    # 3. Surface smoothness (using gradient magnitude as a proxy)
    def compute_smoothness(voxels):
        # Compute gradients along each axis
        grad_x = torch.abs(voxels[:, 1:, :, :] - voxels[:, :-1, :, :]).mean()
        grad_y = torch.abs(voxels[:, :, 1:, :] - voxels[:, :, :-1, :]).mean()
        grad_z = torch.abs(voxels[:, :, :, 1:] - voxels[:, :, :, :-1]).mean()
        return (grad_x + grad_y + grad_z) / 3

    metrics['real_smoothness'] = float(compute_smoothness(real_masks))
    metrics['gen_smoothness'] = float(compute_smoothness(gen_masks))
    metrics['smoothness_diff'] = float(abs(metrics['real_smoothness'] - metrics['gen_smoothness']))

    return metrics


def compute_distribution_metrics(real_samples, generated_samples, only_mask=False):
    """Calculate metrics related to distribution properties"""
    metrics = {}

    # Permute for easier processing
    real_samples = real_samples.permute(0, 2, 3, 4, 1)
    generated_samples = generated_samples.permute(0, 2, 3, 4, 1)

    # 1. Simple L1/L2 distance between mean voxel occupancies
    if only_mask:
        real_data = real_samples[..., 0].flatten(1).numpy()
        gen_data = generated_samples[..., 0].flatten(1).numpy()
    else:
        # For RGBA, calculate distances separately
        real_data = real_samples.flatten(1).numpy()
        gen_data = generated_samples.flatten(1).numpy()

    # Calculate means along batch dimension
    real_mean = np.mean(real_data, axis=0)
    gen_mean = np.mean(gen_data, axis=0)

    # L1 and L2 distances between means
    metrics['l1_dist'] = float(np.mean(np.abs(real_mean - gen_mean)))
    metrics['l2_dist'] = float(np.sqrt(np.mean((real_mean - gen_mean) ** 2)))

    # 2. Variance comparison
    real_var = np.var(real_data, axis=0).mean()
    gen_var = np.var(gen_data, axis=0).mean()
    metrics['variance_ratio'] = float(gen_var / real_var) if real_var > 0 else float('inf')

    return metrics


def compute_classification_metrics(real_labels, generated_labels):
    """Calculate metrics related to class distribution"""
    metrics = {}

    # Convert to numpy arrays
    real_labels = np.array(real_labels)
    generated_labels = np.array(generated_labels)

    # Class distribution similarity
    real_class_hist = np.bincount(real_labels, minlength=max(real_labels) + 1) / len(real_labels)
    gen_class_hist = np.bincount(generated_labels, minlength=max(generated_labels) + 1) / len(generated_labels)

    # Ensure histograms are the same length
    if len(real_class_hist) > len(gen_class_hist):
        gen_class_hist = np.pad(gen_class_hist, (0, len(real_class_hist) - len(gen_class_hist)))
    elif len(gen_class_hist) > len(real_class_hist):
        real_class_hist = np.pad(real_class_hist, (0, len(gen_class_hist) - len(real_class_hist)))

    # Jensen-Shannon divergence between class distributions
    def js_divergence(p, q):
        m = 0.5 * (p + q)
        return 0.5 * (np.sum(p * np.log(p / m + 1e-10)) + np.sum(q * np.log(q / m + 1e-10)))

    metrics['class_js_divergence'] = float(js_divergence(real_class_hist, gen_class_hist))

    return metrics


def evaluate_model(model_path: str, size: int=16, batch_size=16, num_batches=10):
    # Setup
    device = "cuda" if torch.cuda.is_available() else "cpu"
    SIZE = size  # or appropriate size for your model
    only_mask = "only_mask" in model_path

    # Load dataset
    dataset = DummyDataset(length=batch_size * num_batches, size=SIZE)

    # Load model
    model = VoxelUNet(
        model_channels=SIZE,
        in_channels=1 if only_mask else 4,
        out_channels=2 if only_mask else 8,
        channel_mult=(1, 2, 4),
        num_res_blocks=2,
        num_classes=dataset.num_classes,
        attention_resolutions=(2, 4)
    ).to(device)
    model.load_state_dict(torch.load(model_path, weights_only=True))

    # Initialize diffusion
    diffusion = Diffusion(device)

    # Run evaluation
    metrics = evaluate_voxel_quality(
        model=model,
        diffusion=diffusion,
        dataset=dataset,
        batch_size=batch_size,
        num_batches=num_batches,
        only_mask=only_mask,
        device=device
    )

    # Print or log results
    for metric_name, value in metrics.items():
        print(f"{metric_name}: {value}")

    return metrics


if __name__ == '__main__':
    # Add to train_demo.py after training experiments
    print("\nEvaluating model performance...")
    # Evaluate the models with different batch sizes
    metrics = evaluate_model("models/voxel_diffusion_16_3_only_mask_labeled.pth", batch_size=16, num_batches=10)
    print("Evaluation metrics:", metrics)