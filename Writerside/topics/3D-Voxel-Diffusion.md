# 3D Voxel Diffusion

## Abstract
This project proposes a voxel-based 3D diffusion model that progressively adds noise to the input data in a forward process and removes noise via a reverse process, thereby generating complex 3D scenes that adhere to semantic constraints. The method integrates the forward diffusion with reverse generation and employs a conditional generation technique to effectively embed label information. Furthermore, a 3D U-Net architecture is utilized for multi-scale feature extraction and detail restoration. Experimental results demonstrate that the proposed model has high applicability and scalability in tasks such as scene reconstruction, object detection, and molecular property prediction.

## Keywords
3D Diffusion Model, Voxel Representation, Conditional Generation, 3D U-Net, Diffusion Process